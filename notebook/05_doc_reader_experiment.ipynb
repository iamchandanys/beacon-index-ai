{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831a3d67",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ff0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # From PyMuPDF for PDF processing\n",
    "import aiohttp\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_chunks = []\n",
    "file_urls = [\"https://emcdevstoragev2.blob.core.windows.net/document-analysis/42aadc49-c7bf-4055-97d6-b3933554d1a1\"]\n",
    "\n",
    "for file_url in file_urls:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(file_url) as response:\n",
    "            response.raise_for_status()\n",
    "            pdf_bytes = await response.read()\n",
    "\n",
    "    with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            page_text = page.get_text()\n",
    "            doucment = Document(page_content=page_text, metadata={\"page\": page_num})\n",
    "            text_chunks.append(doucment)\n",
    "        \n",
    "print(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a2c8a",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "doc_analyse_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    **You are a data-cleaning assistant.**\n",
    "\n",
    "    I will give you a raw table (possibly from a PDF) that may include:\n",
    "\n",
    "    - Merged or split cells  \n",
    "    - Empty columns or rows  \n",
    "    - Repeated header rows  \n",
    "    - Multi-line entries  \n",
    "    - Mixed logical sections  \n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Your job:\n",
    "\n",
    "    1. **Detect separate sections**  \n",
    "    If the table contains two or more distinct blocks—each with its own header—treat them as separate tables.\n",
    "\n",
    "    2. **Clean each table:**  \n",
    "    - **Drop** any fully empty rows or columns.  \n",
    "    - **Remove** duplicate header rows (keep only the first header in each section).  \n",
    "    - **Flatten** multi-line cells into single lines and trim extra whitespace.  \n",
    "    - **Use** the first line of each section as the header.\n",
    "\n",
    "    3. **Output**  \n",
    "    - For each section produce a JSON array of objects.  \n",
    "    - Use the cleaned header labels (in lowerCamelCase or snake_case) as keys.  \n",
    "    - If there are multiple sections, label each output (e.g. `\"policies\": […]`, `\"errors\": […]`).\n",
    "    - Just return the JSON without any additional text or explanations.\n",
    "    \n",
    "    Here's the raw table data:\n",
    "    {table_content}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azOpenAIllm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "#     ),\n",
    "#     (\"human\", \"I love programming.\"),\n",
    "# ]\n",
    "\n",
    "# azOpenAIllm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40011b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def jsonify_tables(tables: List[List[List[str | None]]]) -> list[str | dict]:\n",
    "    tables_json = []\n",
    "        \n",
    "    for table in tables:\n",
    "        rows = \"\"\n",
    "        \n",
    "        for row in table:\n",
    "            # Filter out empty rows. Example: [\"\", \"name\", \"age\", None, \"city\"] -> [\"name\", \"age\", \"city\"]\n",
    "            filtered_row = [col for col in row if col not in ('', None)]\n",
    "            # Structure the row. Example: \"name | age | city\"\n",
    "            final_row = \" | \".join([x.strip().replace('\\n', '') for x in filtered_row])\n",
    "            # Add the structured row to the rows string\n",
    "            rows += final_row + \"\\n\"\n",
    "            \n",
    "        chain = doc_analyse_prompt | azOpenAIllm\n",
    "        \n",
    "        response = chain.invoke({\n",
    "            \"table_content\": rows\n",
    "        })\n",
    "        \n",
    "        tables_json.append(response.content)\n",
    "        \n",
    "    return tables_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_json(json_data: str) -> str:\n",
    "    content = json_data\n",
    "    \n",
    "    # If the content starts with \"```json\", remove it\n",
    "    if content.strip().startswith(\"```json\"):\n",
    "        content = content.strip()[7:]\n",
    "        \n",
    "        # If the content ends with \"```\", remove it\n",
    "        if content.endswith(\"```\"):\n",
    "            content = content[:-3]\n",
    "        \n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "        return json.dumps(parsed, indent=2, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from langchain.schema import Document\n",
    "     \n",
    "def process_pdf_tables(url: str) -> List[Document]:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with pdfplumber.open(BytesIO(response.content)) as pdf:\n",
    "        doucments = []\n",
    "\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            # JSONify the tables. This will convert the tables into a JSON format that can be processed\n",
    "            j_tables = jsonify_tables(tables)\n",
    "\n",
    "            # Pretty print the JSON tables. This will format the JSON tables for better readability\n",
    "            pretty_tables = [pretty_print_json(tbl) for tbl in j_tables] if j_tables else []\n",
    "\n",
    "            # Add text to the documents list\n",
    "            doucments.append(Document(page_content=text, metadata={\"page\": page.page_number}))\n",
    "            # Add the pretty printed tables to the documents list\n",
    "            [doucments.append(Document(page_content=pt, metadata={\"page\": page.page_number, \"table\": True})) for pt in pretty_tables if pt]\n",
    "            \n",
    "    return doucments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d981f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://emcdevstoragev2.blob.core.windows.net/public/88e6c6c9-afc8-4c6d-bc77-c35bc0af71de.pdf\"\n",
    "        \n",
    "documents = process_pdf_tables(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in documents:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "table_lengths = [len(doc.page_content) for doc in documents if doc.metadata.get(\"table\")]\n",
    "max_table_length = max(table_lengths) if table_lengths else None\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "table_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=max_table_length if max_table_length else 500,\n",
    "    chunk_overlap=(int(max_table_length / 5)) if max_table_length else 100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "splitted_doc: List[Document] = []\n",
    "\n",
    "for doc in documents:\n",
    "    if doc.metadata.get(\"table\"):\n",
    "        # Split the table content using the table splitter\n",
    "        # split_documents returns a list of Document objects. So we extend the splitted_doc list with the result\n",
    "        # Note: appending the result directly to splitted_doc will create a nested list\n",
    "        splitted_doc.extend(table_splitter.split_documents([doc]))\n",
    "    else:\n",
    "        # Split the regular text using the text splitter\n",
    "        # split_documents returns a list of Document objects. So we extend the splitted_doc list with the result\n",
    "        # Note: appending the result directly to splitted_doc will create a nested list\n",
    "        splitted_doc.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "for doc in splitted_doc:\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
