{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf1eb4b",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "This notebook demonstrates a workflow for extracting, processing, and querying information from PDF documents using LangChain and Azure OpenAI services.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Extraction**  \n",
    "    - Load PDF documents using `PyPDFLoader`.\n",
    "    - Each page is treated as a separate `Document`.\n",
    "\n",
    "2. **Data Processing**  \n",
    "    - Split documents into manageable text chunks using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "3. **Data Ingestion**  \n",
    "    - Generate embeddings for text chunks using `AzureOpenAIEmbeddings`.\n",
    "    - Store embeddings in a FAISS vector store for efficient similarity search.\n",
    "\n",
    "4. **Data Retrieval**  \n",
    "    - Use a retriever to find relevant document chunks based on user queries.\n",
    "\n",
    "5. **User Query & RAG (Retrieval-Augmented Generation)**  \n",
    "    - Format retrieved documents as context.\n",
    "    - Use a prompt template and `AzureChatOpenAI` to generate answers based on the context.\n",
    "\n",
    "## Key Variables\n",
    "\n",
    "- `documents`: List of `Document` objects loaded from the PDF.\n",
    "- `splitted_doc`: List of text chunks after splitting.\n",
    "- `azOpenAIembeddings`: Azure OpenAI embeddings model instance.\n",
    "- `vectorstore`: FAISS vector store containing embedded chunks.\n",
    "- `retriever`: Retriever object for similarity search.\n",
    "- `azOpenAIllm`: Azure OpenAI language model instance.\n",
    "- `prompt`: Prompt template for question answering.\n",
    "- `rag_chain`: End-to-end chain for RAG-based question answering.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "- Update the `file_path` variable to point to your PDF file.\n",
    "- Run the notebook cells in order to extract, process, and query your document.\n",
    "- Use the `rag_chain.invoke({\"question\": \"<your question>\"})` to get answers from your document.\n",
    "\n",
    "---\n",
    "For more details, refer to the comments in each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de321c1",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def documents_to_json(docs: list[Document]):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        } for doc in docs\n",
    "    ], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"MLC_user_guide.pdf\")\n",
    "\n",
    "documents = PyPDFLoader(file_path).load() # Each page is a separate document\n",
    "\n",
    "print(f\"Number of pages in the document: {len(documents)}\")\n",
    "print(documents_to_json(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad122fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "splitted_doc = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of chunks after splitting: {len(splitted_doc)}\")\n",
    "print(documents_to_json(splitted_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541e108",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azOpenAIembeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# Example usage of the embeddings\n",
    "query_embeddings = azOpenAIembeddings.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# FAISS is in memory vector store, so it will not persist across sessions\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splitted_doc,\n",
    "    embedding=azOpenAIembeddings\n",
    ")\n",
    "\n",
    "# Example usage of the vector store\n",
    "relavant_docs = vectorstore.similarity_search(\"When will your plan start?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef180793",
   "metadata": {},
   "source": [
    "## 3. Data Retrieval\n",
    "\n",
    "IMPORTANT: We can also convert the vector store into a Retriever object. This makes it easy to integrate with other LangChain methods, as many of them are designed to work with retrievers. Essentially, it serves as a convenient interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": 3})\n",
    "\n",
    "result = retriever.invoke(\"When will your plan start?\")\n",
    "\n",
    "print(documents_to_json(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e49714",
   "metadata": {},
   "source": [
    "## 4. User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "azOpenAIllm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Example usage of the LLM\n",
    "azOpenAIllm.invoke(\"When will your plan start?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    Answer the question based on the context below. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# LCEL: Langchain Core Expression Language\n",
    "# This is a runnable chain that takes the context and question, formats the context, and then\n",
    "# passes it to the prompt, which is then passed to the LLM, and finally parses the output as a string.\n",
    "# The final output is a string that contains the answer to the question based on the context.\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda x: format_docs(retriever.invoke(x[\"question\"]))),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt | azOpenAIllm | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke({\"question\": \"Could you tell me when will your plan start?\"})\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7ce82",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \"\"\"\n",
    "You are an intelligent assistant designed to answer only from the provided context.\n",
    "Do not hallucinate. If unsure, say you dont know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (with a follow-up question):\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=rag_template, input_variables=[\"context\", \"chat_history\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf316876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "def get_chat_history_string(chat_history: list[AIMessage | HumanMessage | SystemMessage]) -> str:\n",
    "    return \"\\n\".join([f\"{msg.__class__.__name__}: {msg.content}\" for msg in chat_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_message: str, chat_history: str) -> str:\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": RunnableLambda(lambda x: format_docs(retriever.invoke(x[\"question\"]))),\n",
    "            \"chat_history\": RunnableLambda(lambda x: x[\"chat_history\"]),\n",
    "            \"question\": RunnableLambda(lambda x: x[\"question\"]),\n",
    "        }\n",
    "        | prompt | azOpenAIllm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    response = rag_chain.invoke({\"question\": user_message, \"chat_history\": chat_history})\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat_history: list[AIMessage | HumanMessage | SystemMessage] = []\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    \n",
    "    if user_message.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_message))\n",
    "    \n",
    "    response = chat(user_message, get_chat_history_string(chat_history))\n",
    "\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bdf17",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Task: Rewrite the user's latest message into a standalone question ONLY if it relies on prior context.\\n\"\n",
    "     \"Context source: You will receive the full conversation as `chat_history` followed by the latest `input`.\\n\\n\"\n",
    "     \"How to use chat_history:\\n\"\n",
    "     \"1) Resolve anaphora/ellipsis (e.g., 'and pricing?', 'that one', 'them', 'it') using entities from chat_history.\\n\"\n",
    "     \"2) Pull ONLY the minimal details from chat_history needed to make the question self-contained.\\n\"\n",
    "     \"3) If chat_history lacks enough info to resolve the reference, return the input unchanged.\\n\\n\"\n",
    "     \"Never rewriteâ€”just return exactly as writtenâ€”when the input is:\\n\"\n",
    "     \"- Greetings/farewells (hi, hello, hey, bye, good morning, etc.)\\n\"\n",
    "     \"- Courtesy/acknowledgment (thanks, thank you, ok/okay, sounds good, cool)\\n\"\n",
    "     \"- Apologies or chit-chat (sorry, no worries)\\n\"\n",
    "     \"- Interjections, fillers, emojis, or punctuation (..., ??, ðŸ‘)\\n\\n\"\n",
    "     \"Additional rules:\\n\"\n",
    "     \"- If the input is already a complete standalone question, return it unchanged.\\n\"\n",
    "     \"- Only rewrite follow-ups that depend on chat_history.\\n\"\n",
    "     \"- Do NOT answer the question; output only the rewritten text.\\n\"\n",
    "     \"- Preserve meaning and tone; do not add new requests.\\n\"\n",
    "     \"- Guardrail: if the input has â‰¤3 words and contains no question mark or why/how term, return unchanged.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "context_qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant that must answer only using the information in the provided context.\\n\"\n",
    "     \"Rules:\\n\"\n",
    "     \"- Begin with a brief, friendly greeting.\\n\"\n",
    "     \"- Use only the provided context; do not rely on outside knowledge or assumptions.\\n\"\n",
    "     \"- If the answer cannot be found in the context, respond exactly with: I don't know.\\n\"\n",
    "     \"- Keep the answer concise and no longer than three sentences total (including the greeting).\\n\"\n",
    "     \"- After the answer, on a new line, ask exactly one short, relevant follow-up question.\\n\"\n",
    "     \"- Still ask the follow-up question even if you replied with 'I don't know.'\\n\"\n",
    "     \"- Use chat_history only to resolve references and maintain continuity.\"\n",
    "    ),\n",
    "    (\"system\", \"Context:\\n{context}\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_v2(user_message: str, chat_history: str) -> str:\n",
    "    # 1) Rewrite user question with chat history context\n",
    "    question_rewriter = (\n",
    "        {\n",
    "            \"input\": RunnableLambda(lambda x: x[\"question\"]), \n",
    "            \"chat_history\": RunnableLambda(lambda x: x[\"chat_history\"])\n",
    "        }\n",
    "        | contextualize_question_prompt | azOpenAIllm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    resultss = question_rewriter.invoke({\"question\": user_message, \"chat_history\": chat_history})\n",
    "\n",
    "    print(resultss)\n",
    "\n",
    "    # 2) Retrieve docs for rewritten question\n",
    "    retrieve_docs = question_rewriter | retriever | format_docs\n",
    "\n",
    "    # 3) Answer using retrieved context + original input + chat history\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retrieve_docs,\n",
    "            \"input\": RunnableLambda(lambda x: x[\"question\"]),\n",
    "            \"chat_history\": RunnableLambda(lambda x: x[\"chat_history\"]),\n",
    "        }\n",
    "        | context_qa_prompt | azOpenAIllm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"question\": user_message,\n",
    "            \"chat_history\": chat_history,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1311f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat_history: list[AIMessage | HumanMessage | SystemMessage] = []\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    \n",
    "    if user_message.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_message))\n",
    "    \n",
    "    response = chat_v2(user_message, chat_history)\n",
    "\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    print(\"---------------\")\n",
    "    print(chat_history)\n",
    "    print(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
