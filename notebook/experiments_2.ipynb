{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de321c1",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")\n",
    "\n",
    "documents = PyPDFLoader(file_path).load() # Each page is a separate document\n",
    "\n",
    "print(f\"Number of pages in the document: {len(documents)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def documents_to_json(docs: list[Document]):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        } for doc in docs\n",
    "    ], indent=2)\n",
    "    \n",
    "print(documents_to_json(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad122fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "splitted_doc = text_splitter.split_documents(documents)\n",
    "\n",
    "# display the number of chunks after splitting\n",
    "print(f\"Number of chunks after splitting: {len(splitted_doc)}\")\n",
    "# display the metadata of the first chunk\n",
    "print(f\"Metadata of first chunk: {splitted_doc[0].metadata}\")\n",
    "# display the content of the first chunk\n",
    "print(f\"Content of first chunk: {splitted_doc[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azOpenAIembeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "azOpenAIembeddings.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# FAISS is in memory vector store, so it will not persist across sessions\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splitted_doc,\n",
    "    embedding=azOpenAIembeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870a4c8",
   "metadata": {},
   "source": [
    "# Retrieval Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relavant_docs = vectorstore.similarity_search(\"who prepared the document?\")\n",
    "# print(documents_to_json(relavant_docs))\n",
    "\n",
    "retriever=vectorstore.as_retriever(search_kwargs = {\"k\": 2})\n",
    "\n",
    "result = retriever.invoke(\"which are the sample package contains?\")\n",
    "\n",
    "print(f\"Result: {result[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "azOpenAIllm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "azOpenAIllm.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prmpt_template = \"\"\"\n",
    "    Answer the question based on the context below. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "user_question = \"who prepared the document?\"\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt = PromptTemplate(template=prmpt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61028ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# LCEL: Langchain Core Expression Language\n",
    "# This is a runnable chain that takes the context and question, formats the context, and then\n",
    "# passes it to the prompt, which is then passed to the LLM, and finally parses the output as a string.\n",
    "# The final output is a string that contains the answer to the question based on the context.\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda x: format_docs(retriever)),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"when is the sample dated?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
