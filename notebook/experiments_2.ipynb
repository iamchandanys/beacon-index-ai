{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf1eb4b",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "This notebook demonstrates a workflow for extracting, processing, and querying information from PDF documents using LangChain and Azure OpenAI services.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Extraction**  \n",
    "    - Load PDF documents using `PyPDFLoader`.\n",
    "    - Each page is treated as a separate `Document`.\n",
    "\n",
    "2. **Data Processing**  \n",
    "    - Split documents into manageable text chunks using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "3. **Data Ingestion**  \n",
    "    - Generate embeddings for text chunks using `AzureOpenAIEmbeddings`.\n",
    "    - Store embeddings in a FAISS vector store for efficient similarity search.\n",
    "\n",
    "4. **Data Retrieval**  \n",
    "    - Use a retriever to find relevant document chunks based on user queries.\n",
    "\n",
    "5. **User Query & RAG (Retrieval-Augmented Generation)**  \n",
    "    - Format retrieved documents as context.\n",
    "    - Use a prompt template and `AzureChatOpenAI` to generate answers based on the context.\n",
    "\n",
    "## Key Variables\n",
    "\n",
    "- `documents`: List of `Document` objects loaded from the PDF.\n",
    "- `splitted_doc`: List of text chunks after splitting.\n",
    "- `azOpenAIembeddings`: Azure OpenAI embeddings model instance.\n",
    "- `vectorstore`: FAISS vector store containing embedded chunks.\n",
    "- `retriever`: Retriever object for similarity search.\n",
    "- `azOpenAIllm`: Azure OpenAI language model instance.\n",
    "- `prompt`: Prompt template for question answering.\n",
    "- `rag_chain`: End-to-end chain for RAG-based question answering.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "- Update the `file_path` variable to point to your PDF file.\n",
    "- Run the notebook cells in order to extract, process, and query your document.\n",
    "- Use the `rag_chain.invoke({\"question\": \"<your question>\"})` to get answers from your document.\n",
    "\n",
    "---\n",
    "For more details, refer to the comments in each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de321c1",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def documents_to_json(docs: list[Document]):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        } for doc in docs\n",
    "    ], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"MLC_user_guide.pdf\")\n",
    "\n",
    "documents = PyPDFLoader(file_path).load() # Each page is a separate document\n",
    "\n",
    "print(f\"Number of pages in the document: {len(documents)}\")\n",
    "print(documents_to_json(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad122fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "splitted_doc = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of chunks after splitting: {len(splitted_doc)}\")\n",
    "print(documents_to_json(splitted_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541e108",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azOpenAIembeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# Example usage of the embeddings\n",
    "query_embeddings = azOpenAIembeddings.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# FAISS is in memory vector store, so it will not persist across sessions\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splitted_doc,\n",
    "    embedding=azOpenAIembeddings\n",
    ")\n",
    "\n",
    "# Example usage of the vector store\n",
    "relavant_docs = vectorstore.similarity_search(\"When will your plan start?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef180793",
   "metadata": {},
   "source": [
    "## 3. Data Retrieval\n",
    "\n",
    "IMPORTANT: We can also convert the vector store into a Retriever object. This makes it easy to integrate with other LangChain methods, as many of them are designed to work with retrievers. Essentially, it serves as a convenient interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": 3})\n",
    "\n",
    "result = retriever.invoke(\"When will your plan start?\")\n",
    "\n",
    "print(documents_to_json(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e49714",
   "metadata": {},
   "source": [
    "## 4. User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "azOpenAIllm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Example usage of the LLM\n",
    "azOpenAIllm.invoke(\"When will your plan start?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    Answer the question based on the context below. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# LCEL: Langchain Core Expression Language\n",
    "# This is a runnable chain that takes the context and question, formats the context, and then\n",
    "# passes it to the prompt, which is then passed to the LLM, and finally parses the output as a string.\n",
    "# The final output is a string that contains the answer to the question based on the context.\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda x: format_docs(retriever.invoke(x[\"question\"]))),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt | azOpenAIllm | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke({\"question\": \"Could you tell me when will your plan start?\"})\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
